<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Database Design for Products | Svādhyāya</title>
<meta name=keywords content="database,api,scaling,product,system design,database design"><meta name=description content="Database Design: A Product-First Approach to Building Scalable Systems over the years wrestling with database architectures I&rsquo;ve learned that the most elegant technical solution isn&rsquo;t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I&rsquo;ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/database-design/><meta name=google-site-verification content="G-Z8CH2E9HK0"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon-1.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/icon-1.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/icon-1.png><link rel=apple-touch-icon href=http://localhost:1313/icon-1.png><link rel=mask-icon href=http://localhost:1313/icon-1.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/database-design/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z8CH2E9HK0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z8CH2E9HK0",{anonymize_ip:!1})}</script><meta property="og:title" content="Database Design for Products"><meta property="og:description" content="Database Design: A Product-First Approach to Building Scalable Systems over the years wrestling with database architectures I&rsquo;ve learned that the most elegant technical solution isn&rsquo;t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I&rsquo;ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/database-design/"><meta property="og:image" content="http://localhost:1313/databasedesign.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-06T01:00:28+05:30"><meta property="article:modified_time" content="2025-09-06T01:00:28+05:30"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/databasedesign.jpg"><meta name=twitter:title content="Database Design for Products"><meta name=twitter:description content="Database Design: A Product-First Approach to Building Scalable Systems over the years wrestling with database architectures I&rsquo;ve learned that the most elegant technical solution isn&rsquo;t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I&rsquo;ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Database Design for Products","item":"http://localhost:1313/posts/database-design/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Database Design for Products","name":"Database Design for Products","description":"Database Design: A Product-First Approach to Building Scalable Systems over the years wrestling with database architectures I\u0026rsquo;ve learned that the most elegant technical solution isn\u0026rsquo;t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I\u0026rsquo;ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve.","keywords":["database","api","scaling","product","system design","database design"],"articleBody":"Database Design: A Product-First Approach to Building Scalable Systems over the years wrestling with database architectures I’ve learned that the most elegant technical solution isn’t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I’ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve. This article distills hard-won lessons from building databases for fantasy sports platforms that needed to handle 100x traffic spikes during Premier League matches, airline systems that couldn’t afford a single booking failure, and financial systems where a single data inconsistency could trigger regulatory audits. The fantasy platform that once struggled with Saturday afternoon traffic surges now gracefully handles millions of users checking their teams during Champions League nights. The airline system processes complex multi-leg journeys without breaking a sweat. Each project taught me something different about the gap between textbook database design and production reality. What changed wasn’t the technology—it was the approach. I stopped designing databases for technical perfection and started designing them for the products they serve. This shift in perspective has been the difference between systems that merely function and systems that excel under pressure.\nThis article shares insights from three distinct use cases, each with unique challenges that shaped my approach to product-centric database design.\n🎯 Product-Centric Database Design Framework 1 2 3 4 5 6 7 8 ┌─────────────────┐ ┌─────────────────┐ ┌────────────────────┐ │ 🎯 Product │───▶│ 🏗️ Schema │───▶│ 🚀 Product │ │ Analysis │ │ Design │ │ Success │ │ │ │ │ │ │ │ • User Workflows│ │ • API Patterns │ │ • User Satisfaction│ │ • Business │ │ • Scale Planning│ │ • Business Goals │ │ Context │ │ │ │ │ └─────────────────┘ └─────────────────┘ └────────────────────┘ Core Principles: ✅ Understand user workflows before schema design\n✅ Design for product roadmap evolution\n✅ Optimize for actual usage patterns\n✅ Measure product outcomes over technical metrics\nThree Case Studies: Different Products, Different Solutions Use Case Core Challenge Key Strategy Primary Focus 🏆 Fantasy Gaming High reads + burst scaling Denormalization + caching Read optimization ✈️ Airline Reservations Schema evolution Version management Flexibility 💰 Loan Management Operational scaling Strategic denormalization Process efficiency Case Study 1: Fantasy Football App - Optimizing for Read-Heavy Workloads The Challenge: 100x Traffic Spikes During Live Matches The fantasy football platform presented a classic read-heavy scenario with extreme burst scaling requirements. During match days, concurrent users jumped from 300/min to 1500+/min (5x spike) as users obsessively checked live scores and player statistics.\n📊 Traffic Pattern Analysis Key Insights:\nRead vs Write Ratio: 15:1 (users check 15x more than they change) Peak Duration: 6-8 hours across multiple match kickoffs Critical Path: Live player statistics during matches Database Architecture: Strategic Denormalization Instead of a normalized approach, I designed for query efficiency with strategic denormalization: The data required during the peak hours could be resolved into three categories:\nPlayer and Team Info - This includes profile information on players and teams Live Updated Data - This is the match data that we are storing for each player at an appearance level. Calculated Metrics and Points - This is the data that is calculated realtime based on the match data and the players role. This is the metrics that is used in ranking players and teams. The first kind of data changes very little and is easier to cache. Invalidating the cache every week or twice a week would be sufficient to ensure that the profile information is always updated. This also reduced the initial load time for our stats dashboard as we could present this data while the rest of the data loaded.\nThe next piece of the puzzle was fetching the match data. We had indexes setup to improve the speed of fetching from the database. The challenge with indexes is that they slow down the writes of the database. To create a balance we used sockets to push real time updates, using Elixir to manage concurrency and indexes on database to manage the fast reads. We preferred to optimise queries over read replicas to ensure the user has access to almost real time data.\nThe third part is where the majority of challenges lie. This is the most critical data and a large part of it is derived from the other sets of data. The derivation process is also based on algorithms created by us and this created the scoring process. This is also to be factored that post the metrics and points calculations these needs to be store to the database for further reads. This could not be solved by one single way so we broke it down to multiple approaches.\nThe first approach was to have our point allocating algorithm in a json format. This allowed us to fetch the points to be allocated at a greater efficiency. The scoring would then be calculated. We would then update the relavant tables with the scores calculated at the previous step. The final retrieval of stats involved real time calculations as well. Like for example the stats and points achieved in home games and away games. To address these kind of scenarios we denormalized the data when storing and stored in seperate tables to make the fetching faster. On the UI as well we made these data available on request. This would ensure there would be zero latency issues for the user. The last step of denormalization requires a tradeoff to use more storage which was an acceptable condition to us as storage is significantly cheaper than cpu cycles and provides us with a better experience for our users. At the same time to minimize storage usage we would archive data that is older than three years. This also keeps our size of the stats tables limited and ensures our existing process remains scalable.\nAPI Query Optimization Examples Before Optimization Common API call: “Get Cristiano Ronaldo’s season summary with home and away points”\nRequired tables: Players → Teams → Match Stats → Game Results → Team Schedules → Opponent Stats Response time: 2-5 seconds Database load: High CPU usage during peak traffic Post Optimization Same API call using seperate homne and away table :\nRequired tables: Single table lookup from player_season_summary Response time: \u003c500ms Database load: Minimal CPU usage Storage trade-off: Duplicate data adds extra storage but improves performance Results Achieved Metric Before After Improvement Peak Load Handling 500 concurrent users per minute 2.5k concurrent users per minute 2k concurrent users per minute API Response Time 3-8 seconds \u003c500ms 6-15x faster Database Load \u003e90% during peaks 40% during peaks \u003e50% reduction Business Gains Smoother user experience Cost reduction due to more efficient queries Case Study 2: Airline Reservation System - Schema Evolution Management The Challenge: NDC Version Compatibility The airline industry’s challenge centered on IATA’s New Distribution Capability (NDC) schema evolution. Airlines implement different NDC versions (17.2, 18.1, 21.3, 24.1) with no standardization, requiring support for multiple versions simultaneously while converting between client-specific versions.\nNDC Version Management Architecture Core Challenge Visualization 1 2 3 4 5 6 7 8 9 10 11 12 13 Client Request (NDC 21.1) ──┐ │ Client Request (NDC 22.2) ──┼──▶ Our System (NDC 22.2 Base) ──┐ │ │ Client Request (NDC 24.1) ──┘ │ ▼ ┌─────────────────────────────┐ │ Version Conversion Engine │ │ │ │ Input: Any NDC Version │ │ Storage: Our Base Version │ │ Output: Client's Version │ └─────────────────────────────┘ Database Schema Design for Version Flexibility The challenge in this case was airlines follow different versions of NDC. The airline data had to be parsed and responded to the client in a our base version. With different schemas we had the challenges of same data in different data structures as well as specific data in higher versions. We came up with the idea of storing the data in the base versions format, with two other tables where we store extra details in a seperate table `version_extras and we would add these based on the version number. The primary table itself would also capture the version details so that the data could be constructed back in the source format. Every time a new version is released we store the extra data in version extensions while maintaining the version info. When we upgrade the base version, we use the version_extras to modify the primary data details. The APIs would follow this pattern to provide the response and this process would make this scalable.\nResults Achieved Metric Result Description Version Support 8+ versions Concurrent NDC version support Migration Time 1 day Time to add new version support Zero Downtime ✓ No service interruption for updates Client Compatibility 100% All client versions supported Business Gains Better support for multiple clients Increasing potential customers Case Study 3: Loan Management System - Scaling for Operational Complexity The Challenge: Evolving Regulatory Requirements The loan management system needed to scale not just for volume, but for operational complexity. As the business evolved—from basic lending to obtaining NBFC license the number of process steps, compliance requirements, and data relationships grew exponentially.\nOperational Lifecycle Process 1 Loan Request → Loan Approval → KYC → Loan Disbursal → EMI Collection → Loan Closure Loan Lifecycle Database Design The loan management system follows a clear lifecycle from application to closure. Here’s the list of the primary database tables that we would use for most queries:\nUser Table - Primary user details that includes sensitive and non sensitive data\nLoan Request Table - Details regarding the requested loan amount, proposed tenure, etc.\nLoan Approval Table - Contains information related to approval amount, approved tenure, rejection_reason(if rejected), etc.\nLoan Disbursal Table - Here we have the financial specifics of the loan - emi_amounts, interest rate, disbursal date, disbursal amount, etc.\nEMI Table - Contains records of each EMI and is updated as each are paid or missed. This also contains records of any emi based penalties.\nLoan Closure Table - The final step when the loan is completed. This table also captures other informations like loan penalties, prepayment based closure, loan_written_off, etc.\nPayment Table - Any kind of payment related transactions\nStrategic Denormalization for Operations Performance Problem: The normalized design required joining multiple tables for a single query to gather any meaningful information for the user, resulting in 3-8 second response times.\nLets take the example of a loan status query: We would need the information of approved amount, loan details, EMIs paid and pending, prepayments made, penalties, etc. This would usually require significant joins and would be a performance bottleneck for the application. A very simple solution is to have the most ferquently used data in a denormalized format in a seperate tables. In this case a Loan Status table which contains the data from multiple tables into this single one. This reduces the time taken for each query improving API performance. This is not a major issue in scale as well because the nature of the application is read heavy.\nQuery Performance Optimization Before (Normalized - Multiple Joins): Loan Status query requiring 6 table joins:\nResponse time: 3-8 seconds at 100 users per minute Query complexity: JOIN across User, Loan_Request, Loan_Approval, Loan_Disbursal, EMI tables After (Denormalized - Single Table): Direct lookup from loan_status table:\nResponse time: \u003c400ms at 100 users per minute Query complexity: Simple SELECT with WHERE clause Results Achieved Metric Before After Improvement Loan Officer Query Time 3-8 seconds \u003c400ms 6-10x faster Compliance Report Generation 2 hours 5 minutes 24x faster Business Gains Faster turnaround for reports Reduced Customer Queries Key Learnings: When to Normalize vs. Denormalize Decision Framework Scenario Approach Reason High Read/Write Ratio (\u003e10:1) Denormalize Query performance over storage Complex Operational Workflows Denormalize Reduce join complexity Transactional Integrity Critical Normalize ACID compliance Rapid Schema Evolution Hybrid Flexibility with performance Strategic Denormalization Guidelines ✅ When to Denormalize:\nQuery patterns are predictable and repetitive Read performance directly impacts user experience Operational workflows require quick decision-making ❌ When to Stay Normalized:\nData consistency is more critical than performance Schema changes frequently Storage costs are a major concern Write patterns are unpredictable Conclusion Database design success comes from understanding your product’s specific needs and user behavior patterns. Each of these three systems succeeded because the database architecture aligned perfectly with the product requirements:\nFantasy Football: Optimized for burst reads and predictable query patterns Airline Reservations: Designed for schema evolution and version compatibility Loan Management: Structured for operational efficiency and regulatory compliance The key insight is that there’s no universal “best” database design—only designs that work well for specific product contexts. Understanding your users’ workflows, your business constraints, and your operational requirements is far more valuable than following theoretical database design rules.\n","wordCount":"2119","inLanguage":"en","image":"http://localhost:1313/databasedesign.jpg","datePublished":"2025-09-06T01:00:28+05:30","dateModified":"2025-09-06T01:00:28+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/database-design/"},"publisher":{"@type":"Organization","name":"Svādhyāya","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon-1.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Svādhyāya (Alt + H)"><img src=http://localhost:1313/icon-1.png alt aria-label=logo height=35>Svādhyāya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Database Design for Products</h1><div class=post-meta><span title='2025-09-06 01:00:28 +0530 IST'>September 6, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2119 words</div></header><div class=post-content><h1 id=database-design-a-product-first-approach-to-building-scalable-systems>Database Design: A Product-First Approach to Building Scalable Systems<a hidden class=anchor aria-hidden=true href=#database-design-a-product-first-approach-to-building-scalable-systems>#</a></h1><p>over the years wrestling with database architectures I&rsquo;ve learned that the most elegant technical solution isn&rsquo;t always the right one. Database design is often viewed through the lens of technical optimization—normalized tables, efficient indexes, and query performance. While these technical aspects are crucial, the database disasters I&rsquo;ve witnessed taught me that the most successful architectures emerge from a deep understanding of the product they serve.
This article distills hard-won lessons from building databases for fantasy sports platforms that needed to handle 100x traffic spikes during Premier League matches, airline systems that couldn&rsquo;t afford a single booking failure, and financial systems where a single data inconsistency could trigger regulatory audits. The fantasy platform that once struggled with Saturday afternoon traffic surges now gracefully handles millions of users checking their teams during Champions League nights. The airline system processes complex multi-leg journeys without breaking a sweat. Each project taught me something different about the gap between textbook database design and production reality.
What changed wasn&rsquo;t the technology—it was the approach. I stopped designing databases for technical perfection and started designing them for the products they serve. This shift in perspective has been the difference between systems that merely function and systems that excel under pressure.</p><p>This article shares insights from three distinct use cases, each with unique challenges that shaped my approach to product-centric database design.</p><h2 id=-product-centric-database-design-framework>🎯 Product-Centric Database Design Framework<a hidden class=anchor aria-hidden=true href=#-product-centric-database-design-framework>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7>7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>┌─────────────────┐    ┌─────────────────┐    ┌────────────────────┐
</span></span><span class=line><span class=cl>│   🎯 Product    │───▶│   🏗️ Schema     │───▶│   🚀 Product       │
</span></span><span class=line><span class=cl>│    Analysis     │    │     Design      │    │    Success         │
</span></span><span class=line><span class=cl>│                 │    │                 │    │                    │
</span></span><span class=line><span class=cl>│ • User Workflows│    │ • API Patterns  │    │ • User Satisfaction│
</span></span><span class=line><span class=cl>│ • Business      │    │ • Scale Planning│    │ • Business Goals   │
</span></span><span class=line><span class=cl>│   Context       │    │                 │    │                    │
</span></span><span class=line><span class=cl>└─────────────────┘    └─────────────────┘    └────────────────────┘
</span></span></code></pre></td></tr></table></div></div><p><strong>Core Principles:</strong>
✅ Understand user workflows before schema design<br>✅ Design for product roadmap evolution<br>✅ Optimize for actual usage patterns<br>✅ Measure product outcomes over technical metrics</p><hr><h2 id=three-case-studies-different-products-different-solutions>Three Case Studies: Different Products, Different Solutions<a hidden class=anchor aria-hidden=true href=#three-case-studies-different-products-different-solutions>#</a></h2><table><thead><tr><th><strong>Use Case</strong></th><th><strong>Core Challenge</strong></th><th><strong>Key Strategy</strong></th><th><strong>Primary Focus</strong></th></tr></thead><tbody><tr><td>🏆 <strong>Fantasy Gaming</strong></td><td>High reads + burst scaling</td><td>Denormalization + caching</td><td>Read optimization</td></tr><tr><td>✈️ <strong>Airline Reservations</strong></td><td>Schema evolution</td><td>Version management</td><td>Flexibility</td></tr><tr><td>💰 <strong>Loan Management</strong></td><td>Operational scaling</td><td>Strategic denormalization</td><td>Process efficiency</td></tr></tbody></table><hr><h1 id=case-study-1-fantasy-football-app---optimizing-for-read-heavy-workloads>Case Study 1: Fantasy Football App - Optimizing for Read-Heavy Workloads<a hidden class=anchor aria-hidden=true href=#case-study-1-fantasy-football-app---optimizing-for-read-heavy-workloads>#</a></h1><h2 id=the-challenge-100x-traffic-spikes-during-live-matches>The Challenge: 100x Traffic Spikes During Live Matches<a hidden class=anchor aria-hidden=true href=#the-challenge-100x-traffic-spikes-during-live-matches>#</a></h2><p>The fantasy football platform presented a classic read-heavy scenario with extreme burst scaling requirements. During match days, concurrent users jumped from 300/min to 1500+/min (5x spike) as users obsessively checked live scores and player statistics.</p><h2 id=-traffic-pattern-analysis>📊 Traffic Pattern Analysis<a hidden class=anchor aria-hidden=true href=#-traffic-pattern-analysis>#</a></h2><p><strong>Key Insights:</strong></p><ul><li><strong>Read vs Write Ratio:</strong> 15:1 (users check 15x more than they change)</li><li><strong>Peak Duration:</strong> 6-8 hours across multiple match kickoffs</li><li><strong>Critical Path:</strong> Live player statistics during matches</li></ul><h2 id=database-architecture-strategic-denormalization>Database Architecture: Strategic Denormalization<a hidden class=anchor aria-hidden=true href=#database-architecture-strategic-denormalization>#</a></h2><p>Instead of a normalized approach, I designed for query efficiency with strategic denormalization:
The data required during the peak hours could be resolved into three categories:</p><ul><li>Player and Team Info - This includes profile information on players and teams</li><li>Live Updated Data - This is the match data that we are storing for each player at an appearance level.</li><li>Calculated Metrics and Points - This is the data that is calculated realtime based on the match data and the players role. This is the metrics that is used in ranking players and teams.</li></ul><p>The first kind of data changes very little and is easier to cache. Invalidating the cache every week or twice a week would be sufficient
to ensure that the profile information is always updated. This also reduced the initial load time for our stats dashboard as we could present this
data while the rest of the data loaded.</p><p>The next piece of the puzzle was fetching the match data. We had indexes setup to improve the speed of fetching from the database. The challenge with indexes is that they slow down the writes of the database. To create a balance we used sockets to push real time updates, using Elixir to manage concurrency and indexes on database to manage the fast reads. We preferred to optimise queries over read replicas to ensure the user has access to almost real time data.</p><p>The third part is where the majority of challenges lie. This is the most critical data and a large part of it is derived from the other sets of data. The derivation process is also based on algorithms created by us and this created the scoring process. This is also to be factored that post the metrics and points calculations these needs to be store to the database for further reads. This could not be solved by one single way so we broke it down to multiple approaches.</p><ol><li>The first approach was to have our point allocating algorithm in a json format. This allowed us to fetch the points to be allocated at a greater efficiency. The scoring would then be calculated.</li><li>We would then update the relavant tables with the scores calculated at the previous step.</li><li>The final retrieval of stats involved real time calculations as well. Like for example the stats and points achieved in home games and away games. To address these kind of scenarios we denormalized the data when storing and stored in seperate tables to make the fetching faster. On the UI as well we made these data available on request. This would ensure there would be zero latency issues for the user.</li></ol><p>The last step of denormalization requires a tradeoff to use more storage which was an acceptable condition to us as storage is significantly cheaper than cpu cycles and provides us with a better experience for our users. At the same time to minimize storage usage we would archive data that is older than three years. This also keeps our size of the stats tables limited and ensures our existing process remains scalable.</p><h2 id=api-query-optimization-examples>API Query Optimization Examples<a hidden class=anchor aria-hidden=true href=#api-query-optimization-examples>#</a></h2><p><strong>Before Optimization</strong>
Common API call: &ldquo;Get Cristiano Ronaldo&rsquo;s season summary with home and away points&rdquo;</p><ul><li>Required tables: Players → Teams → Match Stats → Game Results → Team Schedules → Opponent Stats</li><li>Response time: 2-5 seconds</li><li>Database load: High CPU usage during peak traffic</li></ul><p><strong>Post Optimization</strong>
Same API call using seperate homne and away table :</p><ul><li>Required tables: Single table lookup from player_season_summary</li><li>Response time: &lt;500ms</li><li>Database load: Minimal CPU usage</li><li>Storage trade-off: Duplicate data adds extra storage but improves performance</li></ul><h2 id=results-achieved>Results Achieved<a hidden class=anchor aria-hidden=true href=#results-achieved>#</a></h2><table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>Peak Load Handling</strong></td><td>500 concurrent users per minute</td><td>2.5k concurrent users per minute</td><td>2k concurrent users per minute</td></tr><tr><td><strong>API Response Time</strong></td><td>3-8 seconds</td><td>&lt;500ms</td><td>6-15x faster</td></tr><tr><td><strong>Database Load</strong></td><td>>90% during peaks</td><td>40% during peaks</td><td>>50% reduction</td></tr><tr><td><strong>Business Gains</strong></td><td>Smoother user experience</td><td>Cost reduction due to more efficient queries</td><td></td></tr></tbody></table><hr><p></p><h1 id=case-study-2-airline-reservation-system---schema-evolution-management>Case Study 2: Airline Reservation System - Schema Evolution Management<a hidden class=anchor aria-hidden=true href=#case-study-2-airline-reservation-system---schema-evolution-management>#</a></h1><h2 id=the-challenge-ndc-version-compatibility>The Challenge: NDC Version Compatibility<a hidden class=anchor aria-hidden=true href=#the-challenge-ndc-version-compatibility>#</a></h2><p>The airline industry&rsquo;s challenge centered on IATA&rsquo;s New Distribution Capability (NDC) schema evolution. Airlines implement different NDC versions (17.2, 18.1, 21.3, 24.1) with no standardization, requiring support for multiple versions simultaneously while converting between client-specific versions.</p><h2 id=ndc-version-management-architecture>NDC Version Management Architecture<a hidden class=anchor aria-hidden=true href=#ndc-version-management-architecture>#</a></h2><h3 id=core-challenge-visualization>Core Challenge Visualization<a hidden class=anchor aria-hidden=true href=#core-challenge-visualization>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Client Request (NDC 21.1) ──┐
</span></span><span class=line><span class=cl>                            │
</span></span><span class=line><span class=cl>Client Request (NDC 22.2) ──┼──▶ Our System (NDC 22.2 Base) ──┐
</span></span><span class=line><span class=cl>                            │                                 │
</span></span><span class=line><span class=cl>Client Request (NDC 24.1) ──┘                                 │
</span></span><span class=line><span class=cl>                                                              ▼
</span></span><span class=line><span class=cl>                                           ┌─────────────────────────────┐
</span></span><span class=line><span class=cl>                                           │  Version Conversion Engine  │
</span></span><span class=line><span class=cl>                                           │                             │
</span></span><span class=line><span class=cl>                                           │ Input: Any NDC Version      │
</span></span><span class=line><span class=cl>                                           │ Storage: Our Base Version   │
</span></span><span class=line><span class=cl>                                           │ Output: Client&#39;s Version    │
</span></span><span class=line><span class=cl>                                           └─────────────────────────────┘
</span></span></code></pre></td></tr></table></div></div><h3 id=database-schema-design-for-version-flexibility>Database Schema Design for Version Flexibility<a hidden class=anchor aria-hidden=true href=#database-schema-design-for-version-flexibility>#</a></h3><p>The challenge in this case was airlines follow different versions of NDC. The airline data had to be parsed and responded to the client in a our base version. With different schemas we had the challenges of same data in different data structures as well as specific data in higher versions. We came up with the idea of storing the data in the base versions format, with two other tables where we store extra details in a seperate table `version_extras and we would add these based on the version number. The primary table itself would also capture the version details so that the data could be constructed back in the source format. Every time a new version is released we store the extra data in version extensions while maintaining the version info. When we upgrade the base version, we use the version_extras to modify the primary data details. The APIs would follow this pattern to provide the response and this process would make this scalable.</p><h2 id=results-achieved-1>Results Achieved<a hidden class=anchor aria-hidden=true href=#results-achieved-1>#</a></h2><table><thead><tr><th>Metric</th><th>Result</th><th>Description</th></tr></thead><tbody><tr><td><strong>Version Support</strong></td><td><code>8+ versions</code></td><td>Concurrent NDC version support</td></tr><tr><td><strong>Migration Time</strong></td><td><code>1 day</code></td><td>Time to add new version support</td></tr><tr><td><strong>Zero Downtime</strong></td><td><code>✓</code></td><td>No service interruption for updates</td></tr><tr><td><strong>Client Compatibility</strong></td><td><code>100%</code></td><td>All client versions supported</td></tr><tr><td><strong>Business Gains</strong></td><td>Better support for multiple clients</td><td>Increasing potential customers</td></tr></tbody></table><hr><h1 id=case-study-3-loan-management-system---scaling-for-operational-complexity>Case Study 3: Loan Management System - Scaling for Operational Complexity<a hidden class=anchor aria-hidden=true href=#case-study-3-loan-management-system---scaling-for-operational-complexity>#</a></h1><h2 id=the-challenge-evolving-regulatory-requirements>The Challenge: Evolving Regulatory Requirements<a hidden class=anchor aria-hidden=true href=#the-challenge-evolving-regulatory-requirements>#</a></h2><p>The loan management system needed to scale not just for volume, but for operational complexity. As the business evolved—from basic lending to obtaining NBFC license the number of process steps, compliance requirements, and data relationships grew exponentially.</p><h2 id=operational-lifecycle-process>Operational Lifecycle Process<a hidden class=anchor aria-hidden=true href=#operational-lifecycle-process>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Loan Request → Loan Approval → KYC → Loan Disbursal → EMI Collection → Loan Closure
</span></span></code></pre></td></tr></table></div></div><h3 id=loan-lifecycle-database-design>Loan Lifecycle Database Design<a hidden class=anchor aria-hidden=true href=#loan-lifecycle-database-design>#</a></h3><p>The loan management system follows a clear lifecycle from application to closure. Here&rsquo;s the list of the primary database tables that we would use for most queries:</p><p><strong>User Table</strong> - Primary user details that includes sensitive and non sensitive data</p><p><strong>Loan Request Table</strong> - Details regarding the requested loan amount, proposed tenure, etc.</p><p><strong>Loan Approval Table</strong> - Contains information related to approval amount, approved tenure, rejection_reason(if rejected), etc.</p><p><strong>Loan Disbursal Table</strong> - Here we have the financial specifics of the loan - emi_amounts, interest rate, disbursal date, disbursal amount, etc.</p><p><strong>EMI Table</strong> - Contains records of each EMI and is updated as each are paid or missed. This also contains records of any emi based penalties.</p><p><strong>Loan Closure Table</strong> - The final step when the loan is completed. This table also captures other informations like loan penalties, prepayment based closure, loan_written_off, etc.</p><p><strong>Payment Table</strong> - Any kind of payment related transactions</p><h3 id=strategic-denormalization-for-operations>Strategic Denormalization for Operations<a hidden class=anchor aria-hidden=true href=#strategic-denormalization-for-operations>#</a></h3><hr><p><strong>Performance Problem:</strong>
The normalized design required joining multiple tables for a single query to gather any meaningful information for the user, resulting in 3-8 second response times.</p><p>Lets take the example of a loan status query:
We would need the information of approved amount, loan details, EMIs paid and pending, prepayments made, penalties, etc. This would usually require significant joins and would be a performance bottleneck for the application. A very simple solution is to have the most ferquently used data in a denormalized format in a seperate tables. In this case a Loan Status table which contains the data from multiple tables into this single one. This reduces the time taken for each query improving API performance. This is not a major issue in scale as well because the nature of the application is read heavy.</p><h3 id=query-performance-optimization>Query Performance Optimization<a hidden class=anchor aria-hidden=true href=#query-performance-optimization>#</a></h3><p><strong>Before (Normalized - Multiple Joins):</strong>
Loan Status query requiring 6 table joins:</p><ul><li>Response time: 3-8 seconds at 100 users per minute</li><li>Query complexity: JOIN across User, Loan_Request, Loan_Approval, Loan_Disbursal, EMI tables</li></ul><p><strong>After (Denormalized - Single Table):</strong>
Direct lookup from loan_status table:</p><ul><li>Response time: &lt;400ms at 100 users per minute</li><li>Query complexity: Simple SELECT with WHERE clause</li></ul><h2 id=results-achieved-2>Results Achieved<a hidden class=anchor aria-hidden=true href=#results-achieved-2>#</a></h2><table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>Loan Officer Query Time</strong></td><td>3-8 seconds</td><td>&lt;400ms</td><td>6-10x faster</td></tr><tr><td><strong>Compliance Report Generation</strong></td><td>2 hours</td><td>5 minutes</td><td>24x faster</td></tr><tr><td><strong>Business Gains</strong></td><td>Faster turnaround for reports</td><td>Reduced Customer Queries</td><td></td></tr></tbody></table><hr><h1 id=key-learnings-when-to-normalize-vs-denormalize>Key Learnings: When to Normalize vs. Denormalize<a hidden class=anchor aria-hidden=true href=#key-learnings-when-to-normalize-vs-denormalize>#</a></h1><h2 id=decision-framework>Decision Framework<a hidden class=anchor aria-hidden=true href=#decision-framework>#</a></h2><table><thead><tr><th><strong>Scenario</strong></th><th><strong>Approach</strong></th><th><strong>Reason</strong></th></tr></thead><tbody><tr><td><strong>High Read/Write Ratio (>10:1)</strong></td><td>Denormalize</td><td>Query performance over storage</td></tr><tr><td><strong>Complex Operational Workflows</strong></td><td>Denormalize</td><td>Reduce join complexity</td></tr><tr><td><strong>Transactional Integrity Critical</strong></td><td>Normalize</td><td>ACID compliance</td></tr><tr><td><strong>Rapid Schema Evolution</strong></td><td>Hybrid</td><td>Flexibility with performance</td></tr></tbody></table><h2 id=strategic-denormalization-guidelines>Strategic Denormalization Guidelines<a hidden class=anchor aria-hidden=true href=#strategic-denormalization-guidelines>#</a></h2><p>✅ <strong>When to Denormalize:</strong></p><ul><li>Query patterns are predictable and repetitive</li><li>Read performance directly impacts user experience</li><li>Operational workflows require quick decision-making</li></ul><p>❌ <strong>When to Stay Normalized:</strong></p><ul><li>Data consistency is more critical than performance</li><li>Schema changes frequently</li><li>Storage costs are a major concern</li><li>Write patterns are unpredictable</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Database design success comes from understanding your product&rsquo;s specific needs and user behavior patterns. Each of these three systems succeeded because the database architecture aligned perfectly with the product requirements:</p><ul><li><strong>Fantasy Football:</strong> Optimized for burst reads and predictable query patterns</li><li><strong>Airline Reservations:</strong> Designed for schema evolution and version compatibility</li><li><strong>Loan Management:</strong> Structured for operational efficiency and regulatory compliance</li></ul><p>The key insight is that there&rsquo;s no universal &ldquo;best&rdquo; database design—only designs that work well for specific product contexts. Understanding your users&rsquo; workflows, your business constraints, and your operational requirements is far more valuable than following theoretical database design rules.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/database/>Database</a></li><li><a href=http://localhost:1313/tags/api/>Api</a></li><li><a href=http://localhost:1313/tags/scaling/>Scaling</a></li><li><a href=http://localhost:1313/tags/product/>Product</a></li><li><a href=http://localhost:1313/tags/system-design/>System Design</a></li><li><a href=http://localhost:1313/tags/database-design/>Database Design</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/neovim-with-lua/><span class=title>Next »</span><br><span>Neovim With Lua</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Database Design for Products on x" href="https://x.com/intent/tweet/?text=Database%20Design%20for%20Products&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f&amp;hashtags=database%2capi%2cscaling%2cproduct%2csystemdesign%2cdatabasedesign"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Database Design for Products on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f&amp;title=Database%20Design%20for%20Products&amp;summary=Database%20Design%20for%20Products&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Database Design for Products on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f&title=Database%20Design%20for%20Products"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Database Design for Products on whatsapp" href="https://api.whatsapp.com/send?text=Database%20Design%20for%20Products%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Database Design for Products on ycombinator" href="https://news.ycombinator.com/submitlink?t=Database%20Design%20for%20Products&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fdatabase-design%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://souravbasu-xyz.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><div><span style=margin-right:.5rem;font-size:1.25em ;>Made with ❤️ in 🇮🇳 !</span></div><span>&copy; 2025 <a href=http://localhost:1313/>Svādhyāya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>