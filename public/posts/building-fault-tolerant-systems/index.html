<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Fault Tolerant Systems | Svādhyāya</title><meta name=keywords content><meta name=description content="Building Fault-Tolerant Systems: Lessons from a Production Outage
Introduction
It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn&rsquo;t luxury engineering - it&rsquo;s survival. We had no error tracking, no observability, no automated recovery - nothing."><meta name=author content><link rel=canonical href=https://souravbasu.xyz/posts/building-fault-tolerant-systems/><meta name=google-site-verification content="G-Z8CH2E9HK0"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://souravbasu.xyz/icon-1.png><link rel=icon type=image/png sizes=16x16 href=https://souravbasu.xyz/icon-1.png><link rel=icon type=image/png sizes=32x32 href=https://souravbasu.xyz/icon-1.png><link rel=apple-touch-icon href=https://souravbasu.xyz/icon-1.png><link rel=mask-icon href=https://souravbasu.xyz/icon-1.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://souravbasu.xyz/posts/building-fault-tolerant-systems/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z8CH2E9HK0"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z8CH2E9HK0")}</script><meta property="og:title" content="Building Fault Tolerant Systems"><meta property="og:description" content="Building Fault-Tolerant Systems: Lessons from a Production Outage
Introduction
It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn&rsquo;t luxury engineering - it&rsquo;s survival. We had no error tracking, no observability, no automated recovery - nothing."><meta property="og:type" content="article"><meta property="og:url" content="https://souravbasu.xyz/posts/building-fault-tolerant-systems/"><meta property="og:image" content="https://souravbasu.xyz/fault-tolerant/faulttolerance.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-07T03:36:54+05:30"><meta property="article:modified_time" content="2026-01-07T03:36:54+05:30"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://souravbasu.xyz/fault-tolerant/faulttolerance.png"><meta name=twitter:title content="Building Fault Tolerant Systems"><meta name=twitter:description content="Building Fault-Tolerant Systems: Lessons from a Production Outage
Introduction
It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn&rsquo;t luxury engineering - it&rsquo;s survival. We had no error tracking, no observability, no automated recovery - nothing."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://souravbasu.xyz/posts/"},{"@type":"ListItem","position":2,"name":"Building Fault Tolerant Systems","item":"https://souravbasu.xyz/posts/building-fault-tolerant-systems/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Fault Tolerant Systems","name":"Building Fault Tolerant Systems","description":"Building Fault-Tolerant Systems: Lessons from a Production Outage Introduction It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn\u0026rsquo;t luxury engineering - it\u0026rsquo;s survival. We had no error tracking, no observability, no automated recovery - nothing.\n","keywords":[],"articleBody":"Building Fault-Tolerant Systems: Lessons from a Production Outage Introduction It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn’t luxury engineering - it’s survival. We had no error tracking, no observability, no automated recovery - nothing.\nThe incident cost us more than just 45 minutes. We lost customer trust, faced regulatory scrutiny, and spent the next three months rebuilding our entire infrastructure. But the real lesson wasn’t about the technology - it was about understanding that fault tolerance is the difference between a sustainable business and a ticking time bomb.\nThe cost of downtime varies dramatically by scale. For Stripe, an outage costs approximately $450,000 per minute in lost transaction fees alone. Shopify’s Black Friday downtime could mean millions in lost merchant sales within hours. Amazon loses $220,000 per minute when their site goes down. But for small startups like ours, downtime isn’t just about money - it’s existential. A single prolonged outage can destroy customer trust permanently, trigger contract penalties, and invite competitors to poach your clients.\nWhat I learned from that painful day, and from years of building resilient systems since, is that fault tolerance isn’t about preventing all failures - it’s about building systems that bend without breaking. This article shares industry-proven patterns from Google, AWS, Netflix, and other tech giants that have turned fault tolerance from an aspiration into a science. The Industry-Standard Pillars of Fault Tolerance\nBased on frameworks from Google SRE, AWS Well-Architected, Netflix’s resilience principles, and lessons from countless production systems, fault-tolerant architectures rest on six fundamental pillars. These aren’t theoretical constructs - they’re battle-tested patterns that keep the internet running.\nPillar 1: Redundancy \u0026 High Availability The Core Principle Redundancy eliminates single points of failure through strategic duplication. It’s the foundation of fault tolerance, but implementing it effectively requires understanding the trade-offs between consistency, availability, and cost.\nThe most basic form is multi-instance deployments across availability zones. Netflix pioneered this approach at scale, running every service across at least three AWS availability zones. When AWS US-East-1 experienced a major outage in 2011, Netflix customers barely noticed - their redundant deployments in other zones picked up the load seamlessly.\nKey Implementation Strategies: Multi-zone deployments: Run services across 3+ availability zones minimum Active-active configurations: Stripe processes payments across zones simultaneously for sub-second failover Active-passive setups: Simpler operations for teams that value consistency over speed Geographic distribution: Uber's driver matching runs in 10+ independent regions Geographic distribution takes redundancy global, but it requires careful architectural decisions. Facebook’s multi-datacenter architecture replicates user data across continents, serving their global user base with local latency. The challenge isn’t just having multiple locations - it’s ensuring they can operate independently when network partitions occur.\nDatabase Replication Approaches: PostgreSQL streaming replication for near-real-time disaster recovery MySQL Group Replication for multi-primary write scaling Redis Sentinel for automated cache failover Cassandra or DynamoDB for globally distributed data Essential Tools and Services: Container orchestration: Kubernetes ReplicaSets, Docker Swarm Managed databases: AWS Multi-AZ RDS, Google Cloud SQL, Azure Database Load balancing: AWS ELB, HAProxy, NGINX Plus, Cloudflare Load Balancing The key lesson from years of building redundant systems: start with managed services that handle replication complexity, then customize when you hit their limitations. Your database replication strategy will ultimately determine your uptime ceiling.\nPillar 2: Fault Detection \u0026 Observability You Can’t Fix What You Can’t See\nThis simple truth becomes painfully clear during outages when you’re debugging blind. Modern observability goes beyond traditional monitoring, providing deep visibility into system behavior through three complementary lenses.\nThe Three Pillars of Observability:\nMetrics - Your System’s Vital Signs Prometheus revolutionized metrics collection with its pull-based model and powerful query language. Datadog and New Relic offer managed solutions with sophisticated analytics. CloudWatch serves as the foundation for AWS-based architectures.\nSpotify uses these tools to monitor over 200 microservices, tracking everything from request rates to garbage collection pauses. The key insight: focus on percentiles rather than averages. Your p99 latency tells you what your unluckiest users experience, while averages hide the pain.\nLogs - Your System’s Black Box Industry standards: ELK Stack (Elasticsearch, Logstash, Kibana), Splunk, Grafana Loki Key requirement: Structured logging with correlation IDs across services Slack's achievement: 5-minute incident response through searchable logs Common mistake: Unstructured logs that become expensive noise Traces - Following the Request Journey Distributed tracing reveals the hidden complexity of microservice interactions. Tools like Jaeger and Zipkin, inspired by Google’s Dapper paper, visualize request flow through services. AWS X-Ray provides native tracing for AWS services. GitHub uses distributed tracing to identify bottlenecks, preventing minor slowdowns from cascading into site-wide outages.\nError Tracking: Your Early Warning System Error tracking deserves special attention - it’s what we desperately needed during our outage. Sentry has become the industry standard, capturing exceptions with full stack traces and grouping similar errors intelligently.\nWhere to integrate error tracking:\nApplication layer - catch unhandled exceptions API gateways - monitor failed requests and status codes Background jobs - track task failures and retry patterns Frontend - capture JavaScript errors and user rage clicks Tool ecosystem:\nSentry: Airbnb catches 90% of bugs before user reports Rollbar: Strong DevOps workflow integrations Bugsnag: Mobile and frontend specialist Airbrake: Deep language-specific integrations Google’s Four Golden Signals provide the framework for measurement: Latency (response time distribution), Traffic (request patterns), Errors (failure rates by type), and Saturation (resource utilization). These signals work together - high saturation precedes latency increases, which trigger errors.\nPillar 3: Automated Recovery \u0026 Self-Healing When Humans Can’t Keep Up\nHuman intervention doesn’t scale. When you have 5 servers, you can SSH in and fix things. When you have 500, manual intervention means certain death. Self-healing systems detect failures and recover automatically, often before users notice problems.\nCircuit Breakers: Stop the Cascade Circuit breakers prevent cascade failures by monitoring downstream service health. When failures spike, the circuit “opens,” returning fallback responses instead of propagating failures upstream. Netflix’s Hystrix established the pattern, though modern alternatives like Resilience4j, PyBreaker, and go-breaker have taken over.\nNetflix famously used this pattern during AWS outages - when recommendation services failed, users still saw cached viewing history. The circuit breaker prevented the failure from spreading to core streaming functionality.\nSmart Recovery Patterns: Retries with exponential backoff: Discord achieves 99.99% message delivery Health checks that actually work: Include dependency checks, not just \"200 OK\" Auto-scaling on business metrics: Scale on checkout time, not just CPU Automatic failover: Shopify's zero-downtime Black Friday database switches Kubernetes revolutionized health checking with liveness and readiness probes. Liveness probes restart unhealthy containers, while readiness probes prevent traffic from reaching containers that aren’t ready. The common mistake is shallow health checks that lie about actual service availability.\nAuto-scaling Success Stories: During Prime Day, Amazon’s auto-scaling handles 10x traffic automatically. The sophistication isn’t in the scaling itself - it’s in scaling based on business metrics rather than infrastructure metrics. Scale when customer experience degrades, not when servers get busy.\nDeployment strategies for recovery:\nRolling deployments with automatic rollback on error spike Blue-green deployments for instant version switching Canary deployments testing on 1% of traffic first The golden rule: if deployment requires a war room, you’re doing it wrong.\nPillar 4: Graceful Degradation The Art of Partial Failure Perfect availability is impossible, but complete failure is unacceptable. Graceful degradation ensures that when components fail, the system continues serving core functionality.\nTwitter’s approach during traffic spikes became the textbook example. They disable trending topics, suggestions, and analytics while keeping core tweeting alive. Users might not get the full experience, but they can still communicate - which is Twitter’s core value proposition.\nFeature Flags as Circuit Breakers:\nTools: LaunchDarkly, Split.io, Optimizely, Unleash Implementation: Every feature ships with an off switch Business value: Turn off features, not revenue streams Intelligent Fallback Strategies: LinkedIn switches to cached newsfeeds when recommendation engines struggle. GitHub disables repository search during high load but keeps code operations running. The principle is simple: stale data beats no data, and partial functionality beats complete outage.\nPriority-Based Load Management:\nUber’s system perfectly demonstrates intelligent prioritization:\nP0 (Core): Rider-driver matching - must never fail P1 (Important): Surge pricing - can show cached values P2 (Nice-to-have): Social features - first to be disabled Load shedding takes this further by explicitly rejecting requests when approaching capacity. Google’s approach is to fail fast with proper error codes rather than accept requests that will timeout. Facebook’s proxy layers shed load at the edge, protecting core services from overload.\nPillar 5: Chaos Engineering \u0026 Testing Breaking Things on Purpose to Build Confidence The best way to avoid surprises in production is to create them intentionally. Netflix didn’t just pioneer chaos engineering - they made it a competitive advantage.\nThe Legendary Simian Army:\nNetflix’s tools became industry legend:\nChaos Monkey: Randomly kills instances (runs continuously in production!) Chaos Gorilla: Simulates availability zone failures Chaos Kong: Takes down entire regions Latency Monkey: Injects network delays to find timeout issues The psychological impact is profound. When engineers know Chaos Monkey will kill their instances randomly, they design for failure from the start. It’s not about the tool - it’s about the mindset shift.\nModern Chaos Engineering Ecosystem: The practice has democratized with tools like Gremlin (chaos-as-a-service), LitmusChaos (Kubernetes-native), Azure Chaos Studio, and AWS Fault Injection Simulator. What was once Netflix’s secret weapon is now table stakes for reliable systems.\nBeyond Chaos: Comprehensive Testing\nLoad testing: JMeter, Gatling, k6 - test 10x your peak traffic Game Days: Amazon's weekly failure exercises DiRT exercises: Google's multi-day disaster simulations Synthetic monitoring: Continuous production testing with Pingdom, Datadog Synthetics Facebook’s synthetic monitoring discovered cache dependencies that would have caused cascading failures. The investment in testing transforms real incidents from panic-inducing crises into routine operations.\nIndustry Adoption Milestone: AWS added chaos engineering to Well-Architected Framework (2020), signaling that this is no longer optional for serious systems. Financial services now require chaos testing for regulatory compliance.\nPillar 6: Data Integrity \u0026 Durability Protecting What Can’t Be Replaced\nSystems can be rebuilt in hours. Lost data is gone forever. This fundamental truth drives every decision about data protection.\nThe GitLab Wake-Up Call (2017):\nGitLab had five backup strategies:\nPeriodic PostgreSQL dumps Disk snapshots Continuous replication Offsite backups S3 uploads All five were silently failing. Only luck and a random delayed replica prevented total data loss. The lesson: untested backups aren’t backups - they’re hopes and prayers.\nModern Backup Strategies: Successful backup strategies balance multiple concerns:\nAutomation: Not cron jobs, but orchestrated workflows Geographic distribution: Single region equals single point of failure Testing: Monthly restore drills minimum Retention: Balance compliance requirements with storage costs Data Backfilling: Your Recovery Lifeline When things go wrong, backfilling saves the day. Stripe’s infrastructure processes billions of records during schema migrations without downtime. The key components:\nIdempotent operations that can safely run multiple times Progress tracking with resumable checkpoints Rate limiting to avoid self-inflicted DoS Verification passes to ensure consistency Replication Strategy Selection: Different consistency requirements demand different approaches:\nSynchronous replication (PostgreSQL synchronous standby): Best for financial transactions where zero data loss is non-negotiable, accepting higher write latency as the trade-off.\nAsynchronous replication (MySQL async): Ideal for read-heavy workloads where performance matters more than losing a few seconds of transactions.\nMulti-region active-active (Cassandra, DynamoDB): Perfect for global applications requiring local latency and tunable consistency.\nIndustry Durability Standards:\nAWS S3: 99.999999999% (11 nines) durability Stripe: Payment data in 5 global data centers GitHub: Point-in-time recovery saved them from corruption Your minimum: 3 copies, 2 locations, 1 offsite Case Studies: Learning from Major Outages The most valuable lessons come from studying failures, especially those that affected millions of users and cost millions of dollars.\nAWS S3 Outage (2017) A simple typo in a command took down S3’s control plane for four hours, affecting thousands of services that depended on it. An engineer’s debugging command accidentally removed servers faster than expected, cascading into a complete subsystem restart.\nThe lessons were profound. Blast radius limitation through service isolation could have contained the impact. Even “simple” changes need gradual deployment and validation. Most critically, the incident revealed hidden dependencies between control plane and data plane operations that many services hadn’t considered. Companies learned to architect for S3 unavailability despite its legendary reliability.\nGitHub Outage (2018) A network partition caused a split-brain scenario in GitHub’s database cluster, leading to 24 hours of degraded service. Automated failover systems made decisions that seemed correct locally but were globally inconsistent.\nThe incident challenged assumptions about automated recovery. Sometimes manual intervention by experienced engineers produces better outcomes than automated systems. Partition-tolerant architectures must handle split-brain scenarios explicitly. Comprehensive runbooks and practiced disaster recovery procedures proved more valuable than sophisticated automation. Cloudflare Outage (2019)\nA BGP routing configuration error caused 30 minutes of global impact, taking down millions of websites. A regular expression in a firewall rule consumed excessive CPU, which cascaded into a complete service failure.\nThis highlighted that network-level fault tolerance is as critical as application-level resilience. Configuration management and validation must be as rigorous for infrastructure as for code. Progressive rollout of infrastructure changes, even seemingly minor ones, prevents global impact. Facebook Outage (2021)\nBGP route withdrawal isolated Facebook’s entire infrastructure for over six hours. The configuration change locked engineers out of the very systems needed to fix the problem, creating a circular dependency.\nThe lessons were sobering. Out-of-band access mechanisms must exist for when primary systems fail. Dependencies between control systems can create failure loops. Human access during automation failures requires special consideration - Facebook engineers couldn’t even badge into buildings because the security systems depended on the failed infrastructure.\nConclusion Fault tolerance is achievable at any scale. The fintech startup I mentioned at the beginning now processes millions of transactions daily with 99.99% availability. We didn’t achieve this overnight or with unlimited resources. We built incrementally, starting with observability and error tracking because you can’t fix what you can’t see.\nThe journey began with Sentry for error tracking and basic CloudWatch metrics. We added redundancy incrementally, starting with critical services. We implemented circuit breakers after studying Netflix’s patterns. We introduced chaos engineering gradually, beginning with read-only tests. Each step made the next one easier, creating a virtuous cycle of reliability.\nThe patterns in this article aren’t just for tech giants. Every system can benefit from redundancy, observability, and graceful degradation. The tools mentioned span from open-source solutions suitable for startups to enterprise platforms for large organizations. The key is starting somewhere and improving continuously.\nRemember: the cost of fault tolerance is always less than the cost of downtime. Whether you’re protecting a small startup from extinction or ensuring a global platform’s reliability, these patterns provide a roadmap. Learn from others’ outages to prevent your own. Most importantly, accept that failures will happen and design systems that bend without breaking. That’s not pessimism - it’s engineering.\n","wordCount":"2514","inLanguage":"en","image":"https://souravbasu.xyz/fault-tolerant/faulttolerance.png","datePublished":"2026-01-07T03:36:54+05:30","dateModified":"2026-01-07T03:36:54+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"https://souravbasu.xyz/posts/building-fault-tolerant-systems/"},"publisher":{"@type":"Organization","name":"Svādhyāya","logo":{"@type":"ImageObject","url":"https://souravbasu.xyz/icon-1.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://souravbasu.xyz/ accesskey=h title="Svādhyāya (Alt + H)"><img src=https://souravbasu.xyz/icon-1.png alt aria-label=logo height=35>Svādhyāya</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://souravbasu.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://souravbasu.xyz/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building Fault Tolerant Systems</h1><div class=post-meta><span title='2026-01-07 03:36:54 +0530 IST'>January 7, 2026</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2514 words</div></header><div class=post-content><h1 id=building-fault-tolerant-systems-lessons-from-a-production-outage>Building Fault-Tolerant Systems: Lessons from a Production Outage<a hidden class=anchor aria-hidden=true href=#building-fault-tolerant-systems-lessons-from-a-production-outage>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>It was 12 PM on a weekday - our peak traffic hour. I was one of the three engineers at a small fintech startup. Suddenly, our entire platform went dark. No metrics, no logs, no alerts - just angry customers and a silent monitoring dashboard. We were in limbo and no idea what failed. After 45 minutes of blind debugging and a full system restart, we were back online. That day taught me that fault tolerance isn&rsquo;t luxury engineering - it&rsquo;s survival. We had no error tracking, no observability, no automated recovery - nothing.</p><p>The incident cost us more than just 45 minutes. We lost customer trust, faced regulatory scrutiny, and spent the next three months rebuilding our entire infrastructure. But the real lesson wasn&rsquo;t about the technology - it was about understanding that fault tolerance is the difference between a sustainable business and a ticking time bomb.</p><p>The cost of downtime varies dramatically by scale. For Stripe, an outage costs approximately $450,000 per minute in lost transaction fees alone. Shopify&rsquo;s Black Friday downtime could mean millions in lost merchant sales within hours. Amazon loses $220,000 per minute when their site goes down. But for small startups like ours, downtime isn&rsquo;t just about money - it&rsquo;s existential. A single prolonged outage can destroy customer trust permanently, trigger contract penalties, and invite competitors to poach your clients.</p><p>What I learned from that painful day, and from years of building resilient systems since, is that fault tolerance isn&rsquo;t about preventing all failures - it&rsquo;s about building systems that bend without breaking. This article shares industry-proven patterns from Google, AWS, Netflix, and other tech giants that have turned fault tolerance from an aspiration into a science.
The Industry-Standard Pillars of Fault Tolerance</p><p>Based on frameworks from Google SRE, AWS Well-Architected, Netflix&rsquo;s resilience principles, and lessons from countless production systems, fault-tolerant architectures rest on six fundamental pillars. These aren&rsquo;t theoretical constructs - they&rsquo;re battle-tested patterns that keep the internet running.</p><h3 id=pillar-1-redundancy--high-availability>Pillar 1: Redundancy & High Availability<a hidden class=anchor aria-hidden=true href=#pillar-1-redundancy--high-availability>#</a></h3><h4 id=the-core-principle>The Core Principle<a hidden class=anchor aria-hidden=true href=#the-core-principle>#</a></h4><p>Redundancy eliminates single points of failure through strategic duplication. It&rsquo;s the foundation of fault tolerance, but implementing it effectively requires understanding the trade-offs between consistency, availability, and cost.</p><p>The most basic form is multi-instance deployments across availability zones. Netflix pioneered this approach at scale, running every service across at least three AWS availability zones. When AWS US-East-1 experienced a major outage in 2011, Netflix customers barely noticed - their redundant deployments in other zones picked up the load seamlessly.</p><h4 id=key-implementation-strategies>Key Implementation Strategies:<a hidden class=anchor aria-hidden=true href=#key-implementation-strategies>#</a></h4><pre><code>Multi-zone deployments: Run services across 3+ availability zones minimum
Active-active configurations: Stripe processes payments across zones simultaneously for sub-second failover
Active-passive setups: Simpler operations for teams that value consistency over speed
Geographic distribution: Uber's driver matching runs in 10+ independent regions
</code></pre><p>Geographic distribution takes redundancy global, but it requires careful architectural decisions. Facebook&rsquo;s multi-datacenter architecture replicates user data across continents, serving their global user base with local latency. The challenge isn&rsquo;t just having multiple locations - it&rsquo;s ensuring they can operate independently when network partitions occur.</p><h4 id=database-replication-approaches>Database Replication Approaches:<a hidden class=anchor aria-hidden=true href=#database-replication-approaches>#</a></h4><pre><code>PostgreSQL streaming replication for near-real-time disaster recovery
MySQL Group Replication for multi-primary write scaling
Redis Sentinel for automated cache failover
Cassandra or DynamoDB for globally distributed data
</code></pre><h4 id=essential-tools-and-services>Essential Tools and Services:<a hidden class=anchor aria-hidden=true href=#essential-tools-and-services>#</a></h4><pre><code>Container orchestration: Kubernetes ReplicaSets, Docker Swarm
Managed databases: AWS Multi-AZ RDS, Google Cloud SQL, Azure Database
Load balancing: AWS ELB, HAProxy, NGINX Plus, Cloudflare Load Balancing
</code></pre><p>The key lesson from years of building redundant systems: start with managed services that handle replication complexity, then customize when you hit their limitations. Your database replication strategy will ultimately determine your uptime ceiling.</p><p><img loading=lazy src=../../fault-tolerant/six-pillars.png alt="Six Pillars of Fault Tolerance - hexagon diagram showing interconnected pillars"></p><h3 id=pillar-2-fault-detection--observability>Pillar 2: Fault Detection & Observability<a hidden class=anchor aria-hidden=true href=#pillar-2-fault-detection--observability>#</a></h3><p>You Can&rsquo;t Fix What You Can&rsquo;t See</p><p>This simple truth becomes painfully clear during outages when you&rsquo;re debugging blind. Modern observability goes beyond traditional monitoring, providing deep visibility into system behavior through three complementary lenses.</p><p>The Three Pillars of Observability:</p><h4 id=metrics---your-systems-vital-signs>Metrics - Your System&rsquo;s Vital Signs<a hidden class=anchor aria-hidden=true href=#metrics---your-systems-vital-signs>#</a></h4><p>Prometheus revolutionized metrics collection with its pull-based model and powerful query language. Datadog and New Relic offer managed solutions with sophisticated analytics. CloudWatch serves as the foundation for AWS-based architectures.</p><p>Spotify uses these tools to monitor over 200 microservices, tracking everything from request rates to garbage collection pauses. The key insight: focus on percentiles rather than averages. Your p99 latency tells you what your unluckiest users experience, while averages hide the pain.</p><h4 id=logs---your-systems-black-box>Logs - Your System&rsquo;s Black Box<a hidden class=anchor aria-hidden=true href=#logs---your-systems-black-box>#</a></h4><pre><code>Industry standards: ELK Stack (Elasticsearch, Logstash, Kibana), Splunk, Grafana Loki
Key requirement: Structured logging with correlation IDs across services
Slack's achievement: 5-minute incident response through searchable logs
Common mistake: Unstructured logs that become expensive noise
</code></pre><h4 id=traces---following-the-request-journey>Traces - Following the Request Journey<a hidden class=anchor aria-hidden=true href=#traces---following-the-request-journey>#</a></h4><p>Distributed tracing reveals the hidden complexity of microservice interactions. Tools like Jaeger and Zipkin, inspired by Google&rsquo;s Dapper paper, visualize request flow through services. AWS X-Ray provides native tracing for AWS services. GitHub uses distributed tracing to identify bottlenecks, preventing minor slowdowns from cascading into site-wide outages.</p><h4 id=error-tracking-your-early-warning-system>Error Tracking: Your Early Warning System<a hidden class=anchor aria-hidden=true href=#error-tracking-your-early-warning-system>#</a></h4><p>Error tracking deserves special attention - it&rsquo;s what we desperately needed during our outage. Sentry has become the industry standard, capturing exceptions with full stack traces and grouping similar errors intelligently.</p><p>Where to integrate error tracking:</p><pre><code>Application layer - catch unhandled exceptions
API gateways - monitor failed requests and status codes
Background jobs - track task failures and retry patterns
Frontend - capture JavaScript errors and user rage clicks
</code></pre><p>Tool ecosystem:</p><pre><code>Sentry: Airbnb catches 90% of bugs before user reports
Rollbar: Strong DevOps workflow integrations
Bugsnag: Mobile and frontend specialist
Airbrake: Deep language-specific integrations
</code></pre><p>Google&rsquo;s Four Golden Signals provide the framework for measurement: Latency (response time distribution), Traffic (request patterns), Errors (failure rates by type), and Saturation (resource utilization). These signals work together - high saturation precedes latency increases, which trigger errors.</p><p><img loading=lazy src=../../fault-tolerant/observability.png alt="Observability stack - metrics, logs, traces visualization with tool examples"></p><h3 id=pillar-3-automated-recovery--self-healing>Pillar 3: Automated Recovery & Self-Healing<a hidden class=anchor aria-hidden=true href=#pillar-3-automated-recovery--self-healing>#</a></h3><p>When Humans Can&rsquo;t Keep Up</p><p>Human intervention doesn&rsquo;t scale. When you have 5 servers, you can SSH in and fix things. When you have 500, manual intervention means certain death. Self-healing systems detect failures and recover automatically, often before users notice problems.</p><h4 id=circuit-breakers-stop-the-cascade>Circuit Breakers: Stop the Cascade<a hidden class=anchor aria-hidden=true href=#circuit-breakers-stop-the-cascade>#</a></h4><p>Circuit breakers prevent cascade failures by monitoring downstream service health. When failures spike, the circuit &ldquo;opens,&rdquo; returning fallback responses instead of propagating failures upstream. Netflix&rsquo;s Hystrix established the pattern, though modern alternatives like Resilience4j, PyBreaker, and go-breaker have taken over.</p><p>Netflix famously used this pattern during AWS outages - when recommendation services failed, users still saw cached viewing history. The circuit breaker prevented the failure from spreading to core streaming functionality.</p><p><img loading=lazy src=../../fault-tolerant/circuit-breaker.png alt="Circuit breaker state transitions - closed/open/half-open"></p><h4 id=smart-recovery-patterns>Smart Recovery Patterns:<a hidden class=anchor aria-hidden=true href=#smart-recovery-patterns>#</a></h4><pre><code>Retries with exponential backoff: Discord achieves 99.99% message delivery
Health checks that actually work: Include dependency checks, not just &quot;200 OK&quot;
Auto-scaling on business metrics: Scale on checkout time, not just CPU
Automatic failover: Shopify's zero-downtime Black Friday database switches
</code></pre><p>Kubernetes revolutionized health checking with liveness and readiness probes. Liveness probes restart unhealthy containers, while readiness probes prevent traffic from reaching containers that aren&rsquo;t ready. The common mistake is shallow health checks that lie about actual service availability.</p><h4 id=auto-scaling-success-stories>Auto-scaling Success Stories:<a hidden class=anchor aria-hidden=true href=#auto-scaling-success-stories>#</a></h4><p>During Prime Day, Amazon&rsquo;s auto-scaling handles 10x traffic automatically. The sophistication isn&rsquo;t in the scaling itself - it&rsquo;s in scaling based on business metrics rather than infrastructure metrics. Scale when customer experience degrades, not when servers get busy.</p><p>Deployment strategies for recovery:</p><pre><code>Rolling deployments with automatic rollback on error spike
Blue-green deployments for instant version switching
Canary deployments testing on 1% of traffic first
</code></pre><p>The golden rule: if deployment requires a war room, you&rsquo;re doing it wrong.</p><h3 id=pillar-4-graceful-degradation>Pillar 4: Graceful Degradation<a hidden class=anchor aria-hidden=true href=#pillar-4-graceful-degradation>#</a></h3><h4 id=the-art-of-partial-failure>The Art of Partial Failure<a hidden class=anchor aria-hidden=true href=#the-art-of-partial-failure>#</a></h4><p>Perfect availability is impossible, but complete failure is unacceptable. Graceful degradation ensures that when components fail, the system continues serving core functionality.</p><p>Twitter&rsquo;s approach during traffic spikes became the textbook example. They disable trending topics, suggestions, and analytics while keeping core tweeting alive. Users might not get the full experience, but they can still communicate - which is Twitter&rsquo;s core value proposition.</p><p>Feature Flags as Circuit Breakers:</p><pre><code>Tools: LaunchDarkly, Split.io, Optimizely, Unleash
Implementation: Every feature ships with an off switch
Business value: Turn off features, not revenue streams
</code></pre><h4 id=intelligent-fallback-strategies>Intelligent Fallback Strategies:<a hidden class=anchor aria-hidden=true href=#intelligent-fallback-strategies>#</a></h4><p>LinkedIn switches to cached newsfeeds when recommendation engines struggle. GitHub disables repository search during high load but keeps code operations running. The principle is simple: stale data beats no data, and partial functionality beats complete outage.</p><p>Priority-Based Load Management:</p><p>Uber&rsquo;s system perfectly demonstrates intelligent prioritization:</p><pre><code>P0 (Core): Rider-driver matching - must never fail
P1 (Important): Surge pricing - can show cached values
P2 (Nice-to-have): Social features - first to be disabled
</code></pre><p>Load shedding takes this further by explicitly rejecting requests when approaching capacity. Google&rsquo;s approach is to fail fast with proper error codes rather than accept requests that will timeout. Facebook&rsquo;s proxy layers shed load at the edge, protecting core services from overload.</p><p><img loading=lazy src=../../fault-tolerant/graceful-degradation.png alt="Graceful degradation priority pyramid - P0/P1/P2 tiers"></p><h3 id=pillar-5-chaos-engineering--testing>Pillar 5: Chaos Engineering & Testing<a hidden class=anchor aria-hidden=true href=#pillar-5-chaos-engineering--testing>#</a></h3><h4 id=breaking-things-on-purpose-to-build-confidence>Breaking Things on Purpose to Build Confidence<a hidden class=anchor aria-hidden=true href=#breaking-things-on-purpose-to-build-confidence>#</a></h4><p>The best way to avoid surprises in production is to create them intentionally. Netflix didn&rsquo;t just pioneer chaos engineering - they made it a competitive advantage.</p><p>The Legendary Simian Army:</p><p>Netflix&rsquo;s tools became industry legend:</p><pre><code>Chaos Monkey: Randomly kills instances (runs continuously in production!)
Chaos Gorilla: Simulates availability zone failures
Chaos Kong: Takes down entire regions
Latency Monkey: Injects network delays to find timeout issues
</code></pre><p>The psychological impact is profound. When engineers know Chaos Monkey will kill their instances randomly, they design for failure from the start. It&rsquo;s not about the tool - it&rsquo;s about the mindset shift.</p><p><img loading=lazy src=../../fault-tolerant/simian-army.png alt="Netflix Simian Army hierarchy - from Chaos Monkey to Chaos Kong"></p><h4 id=modern-chaos-engineering-ecosystem>Modern Chaos Engineering Ecosystem:<a hidden class=anchor aria-hidden=true href=#modern-chaos-engineering-ecosystem>#</a></h4><p>The practice has democratized with tools like Gremlin (chaos-as-a-service), LitmusChaos (Kubernetes-native), Azure Chaos Studio, and AWS Fault Injection Simulator. What was once Netflix&rsquo;s secret weapon is now table stakes for reliable systems.</p><p>Beyond Chaos: Comprehensive Testing</p><pre><code>Load testing: JMeter, Gatling, k6 - test 10x your peak traffic
Game Days: Amazon's weekly failure exercises
DiRT exercises: Google's multi-day disaster simulations
Synthetic monitoring: Continuous production testing with Pingdom, Datadog Synthetics
</code></pre><p>Facebook&rsquo;s synthetic monitoring discovered cache dependencies that would have caused cascading failures. The investment in testing transforms real incidents from panic-inducing crises into routine operations.</p><p>Industry Adoption Milestone: AWS added chaos engineering to Well-Architected Framework (2020), signaling that this is no longer optional for serious systems. Financial services now require chaos testing for regulatory compliance.</p><h3 id=pillar-6-data-integrity--durability>Pillar 6: Data Integrity & Durability<a hidden class=anchor aria-hidden=true href=#pillar-6-data-integrity--durability>#</a></h3><p>Protecting What Can&rsquo;t Be Replaced</p><p>Systems can be rebuilt in hours. Lost data is gone forever. This fundamental truth drives every decision about data protection.</p><p>The GitLab Wake-Up Call (2017):</p><p>GitLab had five backup strategies:</p><pre><code>Periodic PostgreSQL dumps
Disk snapshots
Continuous replication
Offsite backups
S3 uploads
</code></pre><p>All five were silently failing. Only luck and a random delayed replica prevented total data loss. The lesson: untested backups aren&rsquo;t backups - they&rsquo;re hopes and prayers.</p><h4 id=modern-backup-strategies>Modern Backup Strategies:<a hidden class=anchor aria-hidden=true href=#modern-backup-strategies>#</a></h4><p>Successful backup strategies balance multiple concerns:</p><pre><code>Automation: Not cron jobs, but orchestrated workflows
Geographic distribution: Single region equals single point of failure
Testing: Monthly restore drills minimum
Retention: Balance compliance requirements with storage costs
</code></pre><h4 id=data-backfilling-your-recovery-lifeline>Data Backfilling: Your Recovery Lifeline<a hidden class=anchor aria-hidden=true href=#data-backfilling-your-recovery-lifeline>#</a></h4><p>When things go wrong, backfilling saves the day. Stripe&rsquo;s infrastructure processes billions of records during schema migrations without downtime. The key components:</p><pre><code>Idempotent operations that can safely run multiple times
Progress tracking with resumable checkpoints
Rate limiting to avoid self-inflicted DoS
Verification passes to ensure consistency
</code></pre><h4 id=replication-strategy-selection>Replication Strategy Selection:<a hidden class=anchor aria-hidden=true href=#replication-strategy-selection>#</a></h4><p>Different consistency requirements demand different approaches:</p><p>Synchronous replication (PostgreSQL synchronous standby): Best for financial transactions where zero data loss is non-negotiable, accepting higher write latency as the trade-off.</p><p>Asynchronous replication (MySQL async): Ideal for read-heavy workloads where performance matters more than losing a few seconds of transactions.</p><p>Multi-region active-active (Cassandra, DynamoDB): Perfect for global applications requiring local latency and tunable consistency.</p><p>Industry Durability Standards:</p><pre><code>AWS S3: 99.999999999% (11 nines) durability
Stripe: Payment data in 5 global data centers
GitHub: Point-in-time recovery saved them from corruption
Your minimum: 3 copies, 2 locations, 1 offsite
</code></pre><h2 id=case-studies-learning-from-major-outages>Case Studies: Learning from Major Outages<a hidden class=anchor aria-hidden=true href=#case-studies-learning-from-major-outages>#</a></h2><p>The most valuable lessons come from studying failures, especially those that affected millions of users and cost millions of dollars.</p><h3 id=aws-s3-outage-2017>AWS S3 Outage (2017)<a hidden class=anchor aria-hidden=true href=#aws-s3-outage-2017>#</a></h3><p>A simple typo in a command took down S3&rsquo;s control plane for four hours, affecting thousands of services that depended on it. An engineer&rsquo;s debugging command accidentally removed servers faster than expected, cascading into a complete subsystem restart.</p><p>The lessons were profound. Blast radius limitation through service isolation could have contained the impact. Even &ldquo;simple&rdquo; changes need gradual deployment and validation. Most critically, the incident revealed hidden dependencies between control plane and data plane operations that many services hadn&rsquo;t considered. Companies learned to architect for S3 unavailability despite its legendary reliability.</p><h3 id=github-outage-2018>GitHub Outage (2018)<a hidden class=anchor aria-hidden=true href=#github-outage-2018>#</a></h3><p>A network partition caused a split-brain scenario in GitHub&rsquo;s database cluster, leading to 24 hours of degraded service. Automated failover systems made decisions that seemed correct locally but were globally inconsistent.</p><p>The incident challenged assumptions about automated recovery. Sometimes manual intervention by experienced engineers produces better outcomes than automated systems. Partition-tolerant architectures must handle split-brain scenarios explicitly. Comprehensive runbooks and practiced disaster recovery procedures proved more valuable than sophisticated automation.
Cloudflare Outage (2019)</p><p>A BGP routing configuration error caused 30 minutes of global impact, taking down millions of websites. A regular expression in a firewall rule consumed excessive CPU, which cascaded into a complete service failure.</p><p>This highlighted that network-level fault tolerance is as critical as application-level resilience. Configuration management and validation must be as rigorous for infrastructure as for code. Progressive rollout of infrastructure changes, even seemingly minor ones, prevents global impact.
Facebook Outage (2021)</p><p>BGP route withdrawal isolated Facebook&rsquo;s entire infrastructure for over six hours. The configuration change locked engineers out of the very systems needed to fix the problem, creating a circular dependency.</p><p>The lessons were sobering. Out-of-band access mechanisms must exist for when primary systems fail. Dependencies between control systems can create failure loops. Human access during automation failures requires special consideration - Facebook engineers couldn&rsquo;t even badge into buildings because the security systems depended on the failed infrastructure.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Fault tolerance is achievable at any scale. The fintech startup I mentioned at the beginning now processes millions of transactions daily with 99.99% availability. We didn&rsquo;t achieve this overnight or with unlimited resources. We built incrementally, starting with observability and error tracking because you can&rsquo;t fix what you can&rsquo;t see.</p><p>The journey began with Sentry for error tracking and basic CloudWatch metrics. We added redundancy incrementally, starting with critical services. We implemented circuit breakers after studying Netflix&rsquo;s patterns. We introduced chaos engineering gradually, beginning with read-only tests. Each step made the next one easier, creating a virtuous cycle of reliability.</p><p>The patterns in this article aren&rsquo;t just for tech giants. Every system can benefit from redundancy, observability, and graceful degradation. The tools mentioned span from open-source solutions suitable for startups to enterprise platforms for large organizations. The key is starting somewhere and improving continuously.</p><p>Remember: the cost of fault tolerance is always less than the cost of downtime. Whether you&rsquo;re protecting a small startup from extinction or ensuring a global platform&rsquo;s reliability, these patterns provide a roadmap. Learn from others&rsquo; outages to prevent your own. Most importantly, accept that failures will happen and design systems that bend without breaking. That&rsquo;s not pessimism - it&rsquo;s engineering.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://souravbasu.xyz/posts/rise-and-rise-of-rust/><span class=title>Next »</span><br><span>Rise and Rise of Rust</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Fault Tolerant Systems on x" href="https://x.com/intent/tweet/?text=Building%20Fault%20Tolerant%20Systems&amp;url=https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Fault Tolerant Systems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f&amp;title=Building%20Fault%20Tolerant%20Systems&amp;summary=Building%20Fault%20Tolerant%20Systems&amp;source=https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Fault Tolerant Systems on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f&title=Building%20Fault%20Tolerant%20Systems"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Fault Tolerant Systems on whatsapp" href="https://api.whatsapp.com/send?text=Building%20Fault%20Tolerant%20Systems%20-%20https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Fault Tolerant Systems on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20Fault%20Tolerant%20Systems&u=https%3a%2f%2fsouravbasu.xyz%2fposts%2fbuilding-fault-tolerant-systems%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://souravbasu-xyz.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><div><span style=margin-right:.5rem;font-size:1.25em ;>Made with ❤️ in 🇮🇳 !</span></div><span>&copy; 2026 <a href=https://souravbasu.xyz/>Svādhyāya</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>